{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, codecs\n",
    "import spacy\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Song</th>\n",
       "      <th>Performer</th>\n",
       "      <th>Year</th>\n",
       "      <th>Decade</th>\n",
       "      <th>Lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eastside</td>\n",
       "      <td>benny blanco, halsey</td>\n",
       "      <td>2019</td>\n",
       "      <td>2010</td>\n",
       "      <td>Uh\\nYeah, yeah\\n\\nWhen I was young, I fell in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wait for you</td>\n",
       "      <td>elliott yamin</td>\n",
       "      <td>2007</td>\n",
       "      <td>2000</td>\n",
       "      <td>I never felt nothing in the world like this be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wildflower</td>\n",
       "      <td>skylark</td>\n",
       "      <td>1973</td>\n",
       "      <td>1970</td>\n",
       "      <td>She's faced the hardest times you could imagin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>even though i'm leaving</td>\n",
       "      <td>luke combs</td>\n",
       "      <td>2019</td>\n",
       "      <td>2010</td>\n",
       "      <td>Daddy, I'm afraid, won't you stay a little whi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>do re mi</td>\n",
       "      <td>blackbear</td>\n",
       "      <td>2017</td>\n",
       "      <td>2010</td>\n",
       "      <td>Do, re, mi, fa, so\\nYeah, yeah, yeah, oh\\nDo, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Song              Performer  Year  Decade  \\\n",
       "0                 eastside  benny blanco, halsey   2019    2010   \n",
       "1             wait for you          elliott yamin  2007    2000   \n",
       "2               wildflower                skylark  1973    1970   \n",
       "3  even though i'm leaving             luke combs  2019    2010   \n",
       "4                 do re mi              blackbear  2017    2010   \n",
       "\n",
       "                                              Lyrics  \n",
       "0  Uh\\nYeah, yeah\\n\\nWhen I was young, I fell in ...  \n",
       "1  I never felt nothing in the world like this be...  \n",
       "2  She's faced the hardest times you could imagin...  \n",
       "3  Daddy, I'm afraid, won't you stay a little whi...  \n",
       "4  Do, re, mi, fa, so\\nYeah, yeah, yeah, oh\\nDo, ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"lyrics_per_song.csv\"\n",
    "df = pd.read_csv(FOLDER + filename)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010_lyrics.txt\n",
      "1990_lyrics.txt\n",
      "1970_lyrics.txt\n",
      "2020_lyrics.txt\n",
      "1960_lyrics.txt\n",
      "1980_lyrics.txt\n",
      "1950_lyrics.txt\n",
      "2000_lyrics.txt\n"
     ]
    }
   ],
   "source": [
    "docs = list()\n",
    "for lyrics_doc in os.listdir(FOLDER):\n",
    "    if \".txt\" in lyrics_doc:\n",
    "        print(lyrics_doc)\n",
    "        with codecs.open(os.path.join(FOLDER,lyrics_doc),encoding=\"utf8\") as f:\n",
    "            docs.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm like the water when your ship rolled in that night\n",
      "Rough on the surface, but you cut through like a knife\n",
      "And if it was an open-shut case\n",
      "I never would've known from that look on your face\n",
      "Lost in your current like a priceless wine\n",
      "\n",
      "The more that you say, the less I know\n",
      "Wherever you stray, I fo\n"
     ]
    }
   ],
   "source": [
    "#preview first lines of 2020_lyrics.txt\n",
    "print(docs[3][0:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm like the water when your ship rolled in that night Rough on the surface, but you cut through like a knife And if it was an open-shut case I never would've known from that look on your face Lost in your current like a priceless wine The more that you say, the less I know Wherever you stray, I fol\n"
     ]
    }
   ],
   "source": [
    "#remove new lines\n",
    "docs = [\" \".join(d.split()) for d in docs]\n",
    "#preview\n",
    "print(docs[3][0:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words: 326\n",
      "First ten stop words: ['of', 'a', 'it', 'themselves', 'now', '’s', 'her', 'some', '‘ll', 'not']\n"
     ]
    }
   ],
   "source": [
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "print('Number of stop words: %d' % len(stopwords))\n",
    "print('First ten stop words:',list(stopwords)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Most common words, without stop words and punctuation:**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "doc = docs[1]\n",
    "text = nlp(doc)\n",
    "tokens = [token.text for token in text]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "stop_words = [token.text for token in text if token.is_stop]\n",
    "words = [token.text for token in text if token.is_stop != True]\n",
    "word_freq = Counter(words)\n",
    "common_words = word_freq.most_common()\n",
    "print(common_words[0:10])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for doc in docs:\n",
    "    text = nlp(doc)\n",
    "    words = [token.text for token in text if token.is_stop != True and token.is_punct != True]\n",
    "\n",
    "    word_freq = Counter(words)\n",
    "    common_words = word_freq.most_common()\n",
    "    \n",
    "    # print five most common tokens\n",
    "    print(f\"{common_words[0:5]}\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for w in [\"like\", \"got\", \"yeah\", \"oh\", \"know\", \"\\n\\n\"]:\n",
    "    nlp.vocab[w].is_stop = True"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "doc = docs[1]\n",
    "text = nlp(doc)\n",
    "words = [token.text for token in text if token.is_stop != True and token.is_punct != True]\n",
    "\n",
    "word_freq = Counter(words)\n",
    "common_words = word_freq.most_common()\n",
    "\n",
    "# print five most common tokens\n",
    "print(f\"{common_words[0:5]}\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for doc in docs:\n",
    "    text = nlp(doc)\n",
    "    words = [token.text for token in text if token.is_stop != True and token.is_punct != True]\n",
    "\n",
    "    word_freq = Counter(words)\n",
    "    common_words = word_freq.most_common()\n",
    "    \n",
    "    # print five most common tokens\n",
    "    print(f\"{common_words[0:5]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /home/ghalifaten/.cache/pip/wheels/2a/e5/c2/fd8dad0a452927c85ecd3c6cbaa4748125246eed73d8303184/empath-0.89-py3-none-any.whl\n",
      "Requirement already satisfied: requests in /home/ghalifaten/Anaconda3/lib/python3.8/site-packages (from empath) (2.24.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ghalifaten/Anaconda3/lib/python3.8/site-packages (from requests->empath) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ghalifaten/Anaconda3/lib/python3.8/site-packages (from requests->empath) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ghalifaten/Anaconda3/lib/python3.8/site-packages (from requests->empath) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ghalifaten/Anaconda3/lib/python3.8/site-packages (from requests->empath) (3.0.4)\n",
      "Installing collected packages: empath\n",
      "Successfully installed empath-0.89\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install empath "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from empath import Empath\n",
    "lexicon = Empath()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(docs[5]) #1980_lyrics.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "empath_features = lexicon.analyze(doc.text,categories = [\"love\", \"pain\", \"sadness\", \"hate\", \"joy\"], normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'love': 0.01418016269870886,\n",
       " 'pain': 0.00802298679005896,\n",
       " 'sadness': 0.002910664974998134,\n",
       " 'hate': 0.002910664974998134,\n",
       " 'joy': 0.0015672811403836108}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empath_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def round_up(value):\n",
    "    return np.round(value*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010: {'love': 0.68, 'pain': 0.47, 'sadness': 0.12, 'hate': 0.21, 'joy': 0.03}\n",
      "\n",
      "1990: {'love': 0.8, 'pain': 0.6, 'sadness': 0.38, 'hate': 0.37, 'joy': 0.17}\n",
      "\n",
      "1970: {'love': 1.09, 'pain': 0.66, 'sadness': 0.17, 'hate': 0.21, 'joy': 0.17}\n",
      "\n",
      "2020: {'love': 0.72, 'pain': 0.3, 'sadness': 0.18, 'hate': 0.12, 'joy': 0.06}\n",
      "\n",
      "1960: {'love': 1.29, 'pain': 0.56, 'sadness': 0.26, 'hate': 0.15, 'joy': 0.09}\n",
      "\n",
      "1980: {'love': 1.42, 'pain': 0.8, 'sadness': 0.29, 'hate': 0.29, 'joy': 0.16}\n",
      "\n",
      "1950: {'love': 1.24, 'pain': 0.28, 'sadness': 0.08, 'hate': 0.06, 'joy': 0.06}\n",
      "\n",
      "2000: {'love': 0.64, 'pain': 0.64, 'sadness': 0.14, 'hate': 0.16, 'joy': 0.08}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decades = [2010,1990,1970,2020,1960,1980,1950,2000] #in the order of \"docs\"\n",
    "for i in range(len(decades)):\n",
    "    doc = nlp(docs[i])\n",
    "    empath_features = lexicon.analyze(doc.text,categories = [\"love\", \"pain\", \"sadness\", \"hate\", \"joy\"], normalize = True)\n",
    "    for key in empath_features:\n",
    "        empath_features[key] = round_up(empath_features[key])§\n",
    "    print('{0}: {1}\\n'.format(decades[i], empath_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
