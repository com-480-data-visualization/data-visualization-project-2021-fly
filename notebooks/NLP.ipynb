{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, codecs\n",
    "import spacy\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Song</th>\n",
       "      <th>Performer</th>\n",
       "      <th>Year</th>\n",
       "      <th>Decade</th>\n",
       "      <th>Lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eastside</td>\n",
       "      <td>benny blanco, halsey</td>\n",
       "      <td>2019</td>\n",
       "      <td>2010</td>\n",
       "      <td>Uh\\nYeah, yeah\\n\\nWhen I was young, I fell in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wait for you</td>\n",
       "      <td>elliott yamin</td>\n",
       "      <td>2007</td>\n",
       "      <td>2000</td>\n",
       "      <td>I never felt nothing in the world like this be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wildflower</td>\n",
       "      <td>skylark</td>\n",
       "      <td>1973</td>\n",
       "      <td>1970</td>\n",
       "      <td>She's faced the hardest times you could imagin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>even though i'm leaving</td>\n",
       "      <td>luke combs</td>\n",
       "      <td>2019</td>\n",
       "      <td>2010</td>\n",
       "      <td>Daddy, I'm afraid, won't you stay a little whi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>do re mi</td>\n",
       "      <td>blackbear</td>\n",
       "      <td>2017</td>\n",
       "      <td>2010</td>\n",
       "      <td>Do, re, mi, fa, so\\nYeah, yeah, yeah, oh\\nDo, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Song              Performer  Year  Decade  \\\n",
       "0                 eastside  benny blanco, halsey   2019    2010   \n",
       "1             wait for you          elliott yamin  2007    2000   \n",
       "2               wildflower                skylark  1973    1970   \n",
       "3  even though i'm leaving             luke combs  2019    2010   \n",
       "4                 do re mi              blackbear  2017    2010   \n",
       "\n",
       "                                              Lyrics  \n",
       "0  Uh\\nYeah, yeah\\n\\nWhen I was young, I fell in ...  \n",
       "1  I never felt nothing in the world like this be...  \n",
       "2  She's faced the hardest times you could imagin...  \n",
       "3  Daddy, I'm afraid, won't you stay a little whi...  \n",
       "4  Do, re, mi, fa, so\\nYeah, yeah, yeah, oh\\nDo, ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"lyrics_per_song.csv\"\n",
    "df = pd.read_csv(FOLDER + filename)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010_lyrics.txt\n",
      "1990_lyrics.txt\n",
      "1970_lyrics.txt\n",
      "2020_lyrics.txt\n",
      "1960_lyrics.txt\n",
      "1980_lyrics.txt\n",
      "1950_lyrics.txt\n",
      "2000_lyrics.txt\n"
     ]
    }
   ],
   "source": [
    "docs = list()\n",
    "for lyrics_doc in os.listdir(FOLDER):\n",
    "    if \".txt\" in lyrics_doc:\n",
    "        print(lyrics_doc)\n",
    "        with codecs.open(os.path.join(FOLDER,lyrics_doc),encoding=\"utf8\") as f:\n",
    "            docs.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm like the water when your ship rolled in that night\n",
      "Rough on the surface, but you cut through like a knife\n",
      "And if it was an open-shut case\n",
      "I never would've known from that look on your face\n",
      "Lost in your current like a priceless wine\n",
      "\n",
      "The more that you say, the less I know\n",
      "Wherever you stray, I fo\n"
     ]
    }
   ],
   "source": [
    "#preview first lines of 2020_lyrics.txt\n",
    "print(docs[3][0:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm like the water when your ship rolled in that night Rough on the surface, but you cut through like a knife And if it was an open-shut case I never would've known from that look on your face Lost in your current like a priceless wine The more that you say, the less I know Wherever you stray, I fol\n"
     ]
    }
   ],
   "source": [
    "#remove new lines\n",
    "docs = [\" \".join(d.split()) for d in docs]\n",
    "#preview\n",
    "print(docs[3][0:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words: 326\n",
      "First ten stop words: ['which', 'hers', 'meanwhile', 'me', 'move', 'your', 'â€™m', 'keep', 'well', 'except']\n"
     ]
    }
   ],
   "source": [
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "print('Number of stop words: %d' % len(stopwords))\n",
    "print('First ten stop words:',list(stopwords)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some more stopwords\n",
    "for w in [\"got\", \"know\", \"\\n\", \"\\n\\n\", \" \"]:\n",
    "    nlp.vocab[w].is_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Most common words, without stop words and punctuation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_words(doc, n):\n",
    "    text = nlp(doc)\n",
    "    words = [token.text for token in text if token.is_stop != True and token.is_punct != True]\n",
    "\n",
    "    word_freq = Counter(words)\n",
    "    common_words = word_freq.most_common()\n",
    "    \n",
    "    # print five most common tokens\n",
    "    print(f\"{common_words[0:n]}\\n\")\n",
    "    return common_words[0:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('yeah', 408), ('oh', 270), ('love', 189), ('gon', 189), ('ft', 178)]\n",
      "\n",
      "[('love', 262), ('baby', 259), ('HOUSE', 166), ('ai', 155), ('time', 136)]\n",
      "\n",
      "[('love', 251), ('oh', 147), ('baby', 126), ('Oh', 125), ('ron', 124)]\n",
      "\n",
      "[('na', 90), ('hope', 31), ('hold', 30), ('baby', 25), ('Yeah', 23)]\n",
      "\n",
      "[('State', 342), ('court', 236), ('love', 228), ('baby', 203), ('suit', 179)]\n",
      "\n",
      "[('love', 282), ('BLOOM', 276), ('PG', 205), ('G', 185), ('Bloom', 154)]\n",
      "\n",
      "[('HARRY', 465), ('says', 392), ('PERRY', 237), ('HARMONY', 214), ('Bloom', 186)]\n",
      "\n",
      "[('love', 222), ('oh', 190), ('girl', 167), ('baby', 166), ('time', 163)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    get_common_words(doc, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.max_length = 2084661"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('\\n', 33442), ('\\n\\n', 1957), ('love', 1573), ('baby', 1003), ('oh', 896), ('yeah', 893), ('Oh', 768), ('time', 720), ('na', 643), ('way', 592), ('gon', 592), ('want', 588), ('right', 570), ('let', 566), ('girl', 563), ('ai', 562), ('little', 541), ('wanna', 514), (' ', 509), ('come', 505), ('man', 487), ('Yeah', 476), ('need', 474), ('HARRY', 465), ('day', 458), ('night', 445), ('says', 445), ('think', 407), ('good', 394), ('said', 388), ('away', 382), ('life', 376), ('feel', 376), ('heart', 374), ('State', 354), ('Bloom', 352), ('tell', 351), ('Come', 347), ('ooh', 344), ('world', 328), ('old', 323), ('eyes', 321), ('ft', 309), ('Baby', 304), ('wo', 302), ('Hey', 296), ('head', 293), ('Let', 292), ('hand', 291), (\"'Cause\", 289), ('thing', 280), ('home', 278), ('BLOOM', 277), ('find', 271), ('court', 266), ('real', 265), ('ta', 262), ('ya', 259), ('face', 240), ('la', 240), ('better', 237), ('PERRY', 237), ('long', 235), ('shit', 223), ('going', 221), ('God', 219), ('case', 219), ('mind', 219), ('things', 217), ('look', 217), ('hear', 216), ('HARMONY', 214), ('song', 211), ('Ooh', 210), ('Got', 208), ('hey', 207), ('bitch', 200), ('suit', 198), ('body', 197), ('Like', 194), ('new', 193), ('round', 193), ('left', 189), ('gone', 188), ('place', 187), ('turn', 187), ('hold', 185), ('2', 183), ('doo', 183), ('music', 182), ('da', 176), ('Love', 175), ('people', 171), ('money', 170), ('Harry', 170), ('true', 168), ('dance', 166), ('fuck', 166), ('bed', 165), ('act', 165)]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('\\n', 33442),\n",
       " ('\\n\\n', 1957),\n",
       " ('love', 1573),\n",
       " ('baby', 1003),\n",
       " ('oh', 896),\n",
       " ('yeah', 893),\n",
       " ('Oh', 768),\n",
       " ('time', 720),\n",
       " ('na', 643),\n",
       " ('way', 592),\n",
       " ('gon', 592),\n",
       " ('want', 588),\n",
       " ('right', 570),\n",
       " ('let', 566),\n",
       " ('girl', 563),\n",
       " ('ai', 562),\n",
       " ('little', 541),\n",
       " ('wanna', 514),\n",
       " (' ', 509),\n",
       " ('come', 505),\n",
       " ('man', 487),\n",
       " ('Yeah', 476),\n",
       " ('need', 474),\n",
       " ('HARRY', 465),\n",
       " ('day', 458),\n",
       " ('night', 445),\n",
       " ('says', 445),\n",
       " ('think', 407),\n",
       " ('good', 394),\n",
       " ('said', 388),\n",
       " ('away', 382),\n",
       " ('life', 376),\n",
       " ('feel', 376),\n",
       " ('heart', 374),\n",
       " ('State', 354),\n",
       " ('Bloom', 352),\n",
       " ('tell', 351),\n",
       " ('Come', 347),\n",
       " ('ooh', 344),\n",
       " ('world', 328),\n",
       " ('old', 323),\n",
       " ('eyes', 321),\n",
       " ('ft', 309),\n",
       " ('Baby', 304),\n",
       " ('wo', 302),\n",
       " ('Hey', 296),\n",
       " ('head', 293),\n",
       " ('Let', 292),\n",
       " ('hand', 291),\n",
       " (\"'Cause\", 289),\n",
       " ('thing', 280),\n",
       " ('home', 278),\n",
       " ('BLOOM', 277),\n",
       " ('find', 271),\n",
       " ('court', 266),\n",
       " ('real', 265),\n",
       " ('ta', 262),\n",
       " ('ya', 259),\n",
       " ('face', 240),\n",
       " ('la', 240),\n",
       " ('better', 237),\n",
       " ('PERRY', 237),\n",
       " ('long', 235),\n",
       " ('shit', 223),\n",
       " ('going', 221),\n",
       " ('God', 219),\n",
       " ('case', 219),\n",
       " ('mind', 219),\n",
       " ('things', 217),\n",
       " ('look', 217),\n",
       " ('hear', 216),\n",
       " ('HARMONY', 214),\n",
       " ('song', 211),\n",
       " ('Ooh', 210),\n",
       " ('Got', 208),\n",
       " ('hey', 207),\n",
       " ('bitch', 200),\n",
       " ('suit', 198),\n",
       " ('body', 197),\n",
       " ('Like', 194),\n",
       " ('new', 193),\n",
       " ('round', 193),\n",
       " ('left', 189),\n",
       " ('gone', 188),\n",
       " ('place', 187),\n",
       " ('turn', 187),\n",
       " ('hold', 185),\n",
       " ('2', 183),\n",
       " ('doo', 183),\n",
       " ('music', 182),\n",
       " ('da', 176),\n",
       " ('Love', 175),\n",
       " ('people', 171),\n",
       " ('money', 170),\n",
       " ('Harry', 170),\n",
       " ('true', 168),\n",
       " ('dance', 166),\n",
       " ('fuck', 166),\n",
       " ('bed', 165),\n",
       " ('act', 165)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with codecs.open(os.path.join(FOLDER,\"all_lyrics.txt\"),encoding=\"utf8\") as f:\n",
    "            all_doc = f.read()\n",
    "get_common_words(all_doc, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
